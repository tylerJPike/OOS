---
title: "Error Analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

After one has created their out-of-sample forecasts, one naturally wants to know how well their models would have historically performed. To facilitate the error analysis and forecast comparison process, OOS contains the `forecast_accuracy` and `forecast_comparison` functions. 

**Forecast accuracy**   
1. Mean Square Error (MSE)  
2. Root Mean Square Error (RMSE)  
3. Mean Absolute Error (MAE)  
4. Mean Absolute Percentage Error (MAPE)  

Notes: All loss functions may estimated simultaneously with the OOS `forecast_accuracy` function, for all forecasts models present in a `forecast_univariate`, `forecast_multivariate`, or `forecast_combine` created long-form data.frame.  

**Forecast comparison**  
1. Forecast Error Ratios    
2. [Diebold-Mariano Test](https://www.sas.upenn.edu/~fdiebold/papers/paper68/pa.dm.pdf) (for unnested models)    
3. [Clark and West Test](https://www.nber.org/papers/t0326) (for nested models)    

Notes: Forecast comparisons may be created with the OOS `forecast_comparison` function, for all forecasts models present in a `forecast_univariate`, `forecast_multivariate`, or `forecast_combine` created long-form data.frame.   
